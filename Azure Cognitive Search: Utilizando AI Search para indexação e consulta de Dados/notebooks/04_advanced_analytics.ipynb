{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be7915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath: notebooks/04_advanced_analytics.ipynb\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# AnÃ¡lise AvanÃ§ada e Machine Learning\\n\",\n",
    "    \"\\n\",\n",
    "    \"Este notebook demonstra tÃ©cnicas avanÃ§adas de anÃ¡lise dos dados indexados, incluindo clustering, anÃ¡lise de sentimentos e detecÃ§Ã£o de padrÃµes.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Objetivos\\n\",\n",
    "    \"- Aplicar tÃ©cnicas de machine learning nos dados indexados\\n\",\n",
    "    \"- Identificar clusters e padrÃµes nos documentos\\n\",\n",
    "    \"- AnÃ¡lise temporal e de tendÃªncias\\n\",\n",
    "    \"- GeraÃ§Ã£o de recomendaÃ§Ãµes inteligentes\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Imports e configuraÃ§Ãµes\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"sys.path.append('..')\\n\",\n",
    "    \"\\n\",\n",
    "    \"from src.search.search_engine import IntelligentSearch\\n\",\n",
    "    \"from src.search.result_processor import SearchResultProcessor\\n\",\n",
    "    \"from config.azure_config import config\\n\",\n",
    "    \"\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Machine Learning imports\\n\",\n",
    "    \"from sklearn.feature_extraction.text import TfidfVectorizer\\n\",\n",
    "    \"from sklearn.cluster import KMeans\\n\",\n",
    "    \"from sklearn.decomposition import PCA\\n\",\n",
    "    \"from sklearn.manifold import TSNE\\n\",\n",
    "    \"from sklearn.metrics.pairwise import cosine_similarity\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Text processing\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"from nltk.corpus import stopwords\\n\",\n",
    "    \"from nltk.tokenize import word_tokenize\\n\",\n",
    "    \"from nltk.stem import SnowballStemmer\\n\",\n",
    "    \"from wordcloud import WordCloud\\n\",\n",
    "    \"\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"from datetime import datetime, timedelta\\n\",\n",
    "    \"from collections import Counter, defaultdict\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Ambiente de anÃ¡lise avanÃ§ada configurado!\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Download NLTK data if needed\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    nltk.data.find('tokenizers/punkt')\\n\",\n",
    "    \"    nltk.data.find('corpora/stopwords')\\n\",\n",
    "    \"except LookupError:\\n\",\n",
    "    \"    print(\\\"Baixando dados NLTK...\\\")\\n\",\n",
    "    \"    nltk.download('punkt')\\n\",\n",
    "    \"    nltk.download('stopwords')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Carregamento e PreparaÃ§Ã£o dos Dados\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Carregar configuraÃ§Ã£o e inicializar search engine\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    with open('../data/processed/index_configuration.json', 'r') as f:\\n\",\n",
    "    \"        index_config = json.load(f)\\n\",\n",
    "    \"    index_name = index_config['index_name']\\n\",\n",
    "    \"except FileNotFoundError:\\n\",\n",
    "    \"    index_name = \\\"intelligent-documents-index\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"search_engine = IntelligentSearch(\\n\",\n",
    "    \"    service_name=config.search_service_name,\\n\",\n",
    "    \"    query_key=config.search_query_key or config.search_admin_key,\\n\",\n",
    "    \"    index_name=index_name\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Buscar todos os documentos para anÃ¡lise\\n\",\n",
    "    \"print(\\\"Carregando documentos para anÃ¡lise...\\\")\\n\",\n",
    "    \"all_docs_result = search_engine.advanced_search(\\n\",\n",
    "    \"    query=\\\"*\\\",\\n\",\n",
    "    \"    top=1000,  # Ajustar conforme necessÃ¡rio\\n\",\n",
    "    \"    facets=[\\\"category\\\", \\\"file_type\\\", \\\"language\\\", \\\"sentiment\\\"]\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if all_docs_result['success']:\\n\",\n",
    "    \"    documents = all_docs_result['documents']\\n\",\n",
    "    \"    total_count = all_docs_result['total_count']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"âœ… Carregados {len(documents)} documentos de {total_count} total\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Converter para DataFrame para anÃ¡lise\\n\",\n",
    "    \"    df = pd.DataFrame(documents)\\n\",\n",
    "    \"    print(f\\\"   Colunas disponÃ­veis: {list(df.columns)}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # EstatÃ­sticas bÃ¡sicas\\n\",\n",
    "    \"    print(f\\\"\\\\nðŸ“Š EstatÃ­sticas BÃ¡sicas:\\\")\\n\",\n",
    "    \"    print(f\\\"   Documentos Ãºnicos: {len(df)}\\\")\\n\",\n",
    "    \"    print(f\\\"   Campos com dados: {df.count().sum()}\\\")\\n\",\n",
    "    \"    print(f\\\"   Campos principais:\\\")\\n\",\n",
    "    \"    for col in ['title', 'content', 'category', 'language', 'sentiment']:\\n\",\n",
    "    \"        if col in df.columns:\\n\",\n",
    "    \"            non_null = df[col].notna().sum()\\n\",\n",
    "    \"            print(f\\\"     {col}: {non_null}/{len(df)} ({non_null/len(df)*100:.1f}%)\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"âŒ Erro ao carregar documentos: {all_docs_result.get('error')}\\\")\\n\",\n",
    "    \"    documents = []\\n\",\n",
    "    \"    df = pd.DataFrame()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. AnÃ¡lise Temporal e TendÃªncias\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if not df.empty and 'modified_date' in df.columns:\\n\",\n",
    "    \"    # Processar datas\\n\",\n",
    "    \"    df['modified_date_parsed'] = pd.to_datetime(df['modified_date'], errors='coerce')\\n\",\n",
    "    \"    df_with_dates = df.dropna(subset=['modified_date_parsed'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(df_with_dates) > 0:\\n\",\n",
    "    \"        print(f\\\"\\\\nðŸ“… AnÃ¡lise Temporal de {len(df_with_dates)} documentos com datas vÃ¡lidas\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Adicionar colunas de tempo\\n\",\n",
    "    \"        df_with_dates['year'] = df_with_dates['modified_date_parsed'].dt.year\\n\",\n",
    "    \"        df_with_dates['month'] = df_with_dates['modified_date_parsed'].dt.month\\n\",\n",
    "    \"        df_with_dates['year_month'] = df_with_dates['modified_date_parsed'].dt.to_period('M')\\n\",\n",
    "    \"        df_with_dates['weekday'] = df_with_dates['modified_date_parsed'].dt.day_name()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # AnÃ¡lise temporal\\n\",\n",
    "    \"        fig = make_subplots(\\n\",\n",
    "    \"            rows=2, cols=2,\\n\",\n",
    "    \"            subplot_titles=('Documentos por MÃªs', 'Documentos por Ano', \\n\",\n",
    "    \"                           'Documentos por Dia da Semana', 'TendÃªncia Temporal'),\\n\",\n",
    "    \"            specs=[[{\\\"type\\\": \\\"scatter\\\"}, {\\\"type\\\": \\\"bar\\\"}],\\n\",\n",
    "    \"                   [{\\\"type\\\": \\\"bar\\\"}, {\\\"type\\\": \\\"scatter\\\"}]]\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # GrÃ¡fico 1: Por mÃªs\\n\",\n",
    "    \"        monthly_counts = df_with_dates['year_month'].value_counts().sort_index()\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Scatter(x=monthly_counts.index.astype(str), y=monthly_counts.values,\\n\",\n",
    "    \"                      mode='lines+markers', name='Por MÃªs'),\\n\",\n",
    "    \"            row=1, col=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # GrÃ¡fico 2: Por ano\\n\",\n",
    "    \"        yearly_counts = df_with_dates['year'].value_counts().sort_index()\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Bar(x=yearly_counts.index, y=yearly_counts.values, name='Por Ano'),\\n\",\n",
    "    \"            row=1, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # GrÃ¡fico 3: Por dia da semana\\n\",\n",
    "    \"        weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\\n\",\n",
    "    \"        weekday_counts = df_with_dates['weekday'].value_counts().reindex(weekday_order, fill_value=0)\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Bar(x=weekday_counts.index, y=weekday_counts.values, name='Por Dia'),\\n\",\n",
    "    \"            row=2, col=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # GrÃ¡fico 4: TendÃªncia com mÃ©dia mÃ³vel\\n\",\n",
    "    \"        daily_counts = df_with_dates.groupby(df_with_dates['modified_date_parsed'].dt.date).size()\\n\",\n",
    "    \"        daily_counts = daily_counts.reindex(pd.date_range(daily_counts.index.min(), \\n\",\n",
    "    \"                                                          daily_counts.index.max()), fill_value=0)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # MÃ©dia mÃ³vel de 7 dias\\n\",\n",
    "    \"        rolling_mean = daily_counts.rolling(window=7, center=True).mean()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Scatter(x=daily_counts.index, y=daily_counts.values,\\n\",\n",
    "    \"                      mode='markers', name='DiÃ¡rio', opacity=0.6),\\n\",\n",
    "    \"            row=2, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Scatter(x=rolling_mean.index, y=rolling_mean.values,\\n\",\n",
    "    \"                      mode='lines', name='MÃ©dia MÃ³vel 7d'),\\n\",\n",
    "    \"            row=2, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.update_layout(height=800, title_text=\\\"AnÃ¡lise Temporal dos Documentos\\\")\\n\",\n",
    "    \"        fig.show()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Insights temporais\\n\",\n",
    "    \"        print(f\\\"\\\\nðŸ” Insights Temporais:\\\")\\n\",\n",
    "    \"        print(f\\\"   PerÃ­odo analisado: {df_with_dates['modified_date_parsed'].min().date()} a {df_with_dates['modified_date_parsed'].max().date()}\\\")\\n\",\n",
    "    \"        print(f\\\"   MÃªs com mais documentos: {monthly_counts.idxmax()} ({monthly_counts.max()} docs)\\\")\\n\",\n",
    "    \"        print(f\\\"   Dia da semana mais comum: {weekday_counts.idxmax()} ({weekday_counts.max()} docs)\\\")\\n\",\n",
    "    \"        print(f\\\"   MÃ©dia de documentos por dia: {daily_counts.mean():.1f}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"âš ï¸ Nenhuma data vÃ¡lida encontrada para anÃ¡lise temporal\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"âš ï¸ Campo 'modified_date' nÃ£o disponÃ­vel para anÃ¡lise temporal\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. AnÃ¡lise de Clustering de Documentos\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if not df.empty and 'content' in df.columns:\\n\",\n",
    "    \"    # Preparar textos para anÃ¡lise\\n\",\n",
    "    \"    texts = df['content'].fillna('').astype(str)\\n\",\n",
    "    \"    valid_texts = [text for text in texts if len(text.strip()) > 50]  # Filtrar textos muito curtos\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nðŸ”¤ Preparando {len(valid_texts)} textos para clustering...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(valid_texts) >= 10:  # Precisa de pelo menos 10 documentos\\n\",\n",
    "    \"        # Configurar TF-IDF\\n\",\n",
    "    \"        portuguese_stopwords = set(stopwords.words('portuguese'))\\n\",\n",
    "    \"        english_stopwords = set(stopwords.words('english'))\\n\",\n",
    "    \"        all_stopwords = portuguese_stopwords.union(english_stopwords)\\n# filepath: notebooks/04_advanced_analytics.ipynb\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# AnÃ¡lise AvanÃ§ada e Machine Learning\\n\",\n",
    "    \"\\n\",\n",
    "    \"Este notebook demonstra tÃ©cnicas avanÃ§adas de anÃ¡lise dos dados indexados, incluindo clustering, anÃ¡lise de sentimentos e detecÃ§Ã£o de padrÃµes.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Objetivos\\n\",\n",
    "    \"- Aplicar tÃ©cnicas de machine learning nos dados indexados\\n\",\n",
    "    \"- Identificar clusters e padrÃµes nos documentos\\n\",\n",
    "    \"- AnÃ¡lise temporal e de tendÃªncias\\n\",\n",
    "    \"- GeraÃ§Ã£o de recomendaÃ§Ãµes inteligentes\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Imports e configuraÃ§Ãµes\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"sys.path.append('..')\\n\",\n",
    "    \"\\n\",\n",
    "    \"from src.search.search_engine import IntelligentSearch\\n\",\n",
    "    \"from src.search.result_processor import SearchResultProcessor\\n\",\n",
    "    \"from config.azure_config import config\\n\",\n",
    "    \"\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Machine Learning imports\\n\",\n",
    "    \"from sklearn.feature_extraction.text import TfidfVectorizer\\n\",\n",
    "    \"from sklearn.cluster import KMeans\\n\",\n",
    "    \"from sklearn.decomposition import PCA\\n\",\n",
    "    \"from sklearn.manifold import TSNE\\n\",\n",
    "    \"from sklearn.metrics.pairwise import cosine_similarity\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Text processing\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"from nltk.corpus import stopwords\\n\",\n",
    "    \"from nltk.tokenize import word_tokenize\\n\",\n",
    "    \"from nltk.stem import SnowballStemmer\\n\",\n",
    "    \"from wordcloud import WordCloud\\n\",\n",
    "    \"\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"from datetime import datetime, timedelta\\n\",\n",
    "    \"from collections import Counter, defaultdict\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Ambiente de anÃ¡lise avanÃ§ada configurado!\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Download NLTK data if needed\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    nltk.data.find('tokenizers/punkt')\\n\",\n",
    "    \"    nltk.data.find('corpora/stopwords')\\n\",\n",
    "    \"except LookupError:\\n\",\n",
    "    \"    print(\\\"Baixando dados NLTK...\\\")\\n\",\n",
    "    \"    nltk.download('punkt')\\n\",\n",
    "    \"    nltk.download('stopwords')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Carregamento e PreparaÃ§Ã£o dos Dados\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Carregar configuraÃ§Ã£o e inicializar search engine\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    with open('../data/processed/index_configuration.json', 'r') as f:\\n\",\n",
    "    \"        index_config = json.load(f)\\n\",\n",
    "    \"    index_name = index_config['index_name']\\n\",\n",
    "    \"except FileNotFoundError:\\n\",\n",
    "    \"    index_name = \\\"intelligent-documents-index\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"search_engine = IntelligentSearch(\\n\",\n",
    "    \"    service_name=config.search_service_name,\\n\",\n",
    "    \"    query_key=config.search_query_key or config.search_admin_key,\\n\",\n",
    "    \"    index_name=index_name\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Buscar todos os documentos para anÃ¡lise\\n\",\n",
    "    \"print(\\\"Carregando documentos para anÃ¡lise...\\\")\\n\",\n",
    "    \"all_docs_result = search_engine.advanced_search(\\n\",\n",
    "    \"    query=\\\"*\\\",\\n\",\n",
    "    \"    top=1000,  # Ajustar conforme necessÃ¡rio\\n\",\n",
    "    \"    facets=[\\\"category\\\", \\\"file_type\\\", \\\"language\\\", \\\"sentiment\\\"]\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if all_docs_result['success']:\\n\",\n",
    "    \"    documents = all_docs_result['documents']\\n\",\n",
    "    \"    total_count = all_docs_result['total_count']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"âœ… Carregados {len(documents)} documentos de {total_count} total\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Converter para DataFrame para anÃ¡lise\\n\",\n",
    "    \"    df = pd.DataFrame(documents)\\n\",\n",
    "    \"    print(f\\\"   Colunas disponÃ­veis: {list(df.columns)}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # EstatÃ­sticas bÃ¡sicas\\n\",\n",
    "    \"    print(f\\\"\\\\nðŸ“Š EstatÃ­sticas BÃ¡sicas:\\\")\\n\",\n",
    "    \"    print(f\\\"   Documentos Ãºnicos: {len(df)}\\\")\\n\",\n",
    "    \"    print(f\\\"   Campos com dados: {df.count().sum()}\\\")\\n\",\n",
    "    \"    print(f\\\"   Campos principais:\\\")\\n\",\n",
    "    \"    for col in ['title', 'content', 'category', 'language', 'sentiment']:\\n\",\n",
    "    \"        if col in df.columns:\\n\",\n",
    "    \"            non_null = df[col].notna().sum()\\n\",\n",
    "    \"            print(f\\\"     {col}: {non_null}/{len(df)} ({non_null/len(df)*100:.1f}%)\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"âŒ Erro ao carregar documentos: {all_docs_result.get('error')}\\\")\\n\",\n",
    "    \"    documents = []\\n\",\n",
    "    \"    df = pd.DataFrame()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. AnÃ¡lise Temporal e TendÃªncias\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if not df.empty and 'modified_date' in df.columns:\\n\",\n",
    "    \"    # Processar datas\\n\",\n",
    "    \"    df['modified_date_parsed'] = pd.to_datetime(df['modified_date'], errors='coerce')\\n\",\n",
    "    \"    df_with_dates = df.dropna(subset=['modified_date_parsed'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(df_with_dates) > 0:\\n\",\n",
    "    \"        print(f\\\"\\\\nðŸ“… AnÃ¡lise Temporal de {len(df_with_dates)} documentos com datas vÃ¡lidas\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Adicionar colunas de tempo\\n\",\n",
    "    \"        df_with_dates['year'] = df_with_dates['modified_date_parsed'].dt.year\\n\",\n",
    "    \"        df_with_dates['month'] = df_with_dates['modified_date_parsed'].dt.month\\n\",\n",
    "    \"        df_with_dates['year_month'] = df_with_dates['modified_date_parsed'].dt.to_period('M')\\n\",\n",
    "    \"        df_with_dates['weekday'] = df_with_dates['modified_date_parsed'].dt.day_name()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # AnÃ¡lise temporal\\n\",\n",
    "    \"        fig = make_subplots(\\n\",\n",
    "    \"            rows=2, cols=2,\\n\",\n",
    "    \"            subplot_titles=('Documentos por MÃªs', 'Documentos por Ano', \\n\",\n",
    "    \"                           'Documentos por Dia da Semana', 'TendÃªncia Temporal'),\\n\",\n",
    "    \"            specs=[[{\\\"type\\\": \\\"scatter\\\"}, {\\\"type\\\": \\\"bar\\\"}],\\n\",\n",
    "    \"                   [{\\\"type\\\": \\\"bar\\\"}, {\\\"type\\\": \\\"scatter\\\"}]]\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # GrÃ¡fico 1: Por mÃªs\\n\",\n",
    "    \"        monthly_counts = df_with_dates['year_month'].value_counts().sort_index()\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Scatter(x=monthly_counts.index.astype(str), y=monthly_counts.values,\\n\",\n",
    "    \"                      mode='lines+markers', name='Por MÃªs'),\\n\",\n",
    "    \"            row=1, col=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # GrÃ¡fico 2: Por ano\\n\",\n",
    "    \"        yearly_counts = df_with_dates['year'].value_counts().sort_index()\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Bar(x=yearly_counts.index, y=yearly_counts.values, name='Por Ano'),\\n\",\n",
    "    \"            row=1, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # GrÃ¡fico 3: Por dia da semana\\n\",\n",
    "    \"        weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\\n\",\n",
    "    \"        weekday_counts = df_with_dates['weekday'].value_counts().reindex(weekday_order, fill_value=0)\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Bar(x=weekday_counts.index, y=weekday_counts.values, name='Por Dia'),\\n\",\n",
    "    \"            row=2, col=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # GrÃ¡fico 4: TendÃªncia com mÃ©dia mÃ³vel\\n\",\n",
    "    \"        daily_counts = df_with_dates.groupby(df_with_dates['modified_date_parsed'].dt.date).size()\\n\",\n",
    "    \"        daily_counts = daily_counts.reindex(pd.date_range(daily_counts.index.min(), \\n\",\n",
    "    \"                                                          daily_counts.index.max()), fill_value=0)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # MÃ©dia mÃ³vel de 7 dias\\n\",\n",
    "    \"        rolling_mean = daily_counts.rolling(window=7, center=True).mean()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Scatter(x=daily_counts.index, y=daily_counts.values,\\n\",\n",
    "    \"                      mode='markers', name='DiÃ¡rio', opacity=0.6),\\n\",\n",
    "    \"            row=2, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Scatter(x=rolling_mean.index, y=rolling_mean.values,\\n\",\n",
    "    \"                      mode='lines', name='MÃ©dia MÃ³vel 7d'),\\n\",\n",
    "    \"            row=2, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.update_layout(height=800, title_text=\\\"AnÃ¡lise Temporal dos Documentos\\\")\\n\",\n",
    "    \"        fig.show()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Insights temporais\\n\",\n",
    "    \"        print(f\\\"\\\\nðŸ” Insights Temporais:\\\")\\n\",\n",
    "    \"        print(f\\\"   PerÃ­odo analisado: {df_with_dates['modified_date_parsed'].min().date()} a {df_with_dates['modified_date_parsed'].max().date()}\\\")\\n\",\n",
    "    \"        print(f\\\"   MÃªs com mais documentos: {monthly_counts.idxmax()} ({monthly_counts.max()} docs)\\\")\\n\",\n",
    "    \"        print(f\\\"   Dia da semana mais comum: {weekday_counts.idxmax()} ({weekday_counts.max()} docs)\\\")\\n\",\n",
    "    \"        print(f\\\"   MÃ©dia de documentos por dia: {daily_counts.mean():.1f}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"âš ï¸ Nenhuma data vÃ¡lida encontrada para anÃ¡lise temporal\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"âš ï¸ Campo 'modified_date' nÃ£o disponÃ­vel para anÃ¡lise temporal\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. AnÃ¡lise de Clustering de Documentos\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if not df.empty and 'content' in df.columns:\\n\",\n",
    "    \"    # Preparar textos para anÃ¡lise\\n\",\n",
    "    \"    texts = df['content'].fillna('').astype(str)\\n\",\n",
    "    \"    valid_texts = [text for text in texts if len(text.strip()) > 50]  # Filtrar textos muito curtos\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\nðŸ”¤ Preparando {len(valid_texts)} textos para clustering...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(valid_texts) >= 10:  # Precisa de pelo menos 10 documentos\\n\",\n",
    "    \"        # Configurar TF-IDF\\n\",\n",
    "    \"        portuguese_stopwords = set(stopwords.words('portuguese'))\\n\",\n",
    "    \"        english_stopwords = set(stopwords.words('english'))\\n\",\n",
    "    \"        all_stopwords = portuguese_stopwords.union(english_stopwords)\\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
