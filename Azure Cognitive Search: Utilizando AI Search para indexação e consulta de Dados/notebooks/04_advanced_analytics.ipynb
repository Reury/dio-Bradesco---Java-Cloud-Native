{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2be7915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath: notebooks/04_advanced_analytics.ipynb\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Análise Avançada e Machine Learning\\n\",\n",
    "    \"\\n\",\n",
    "    \"Este notebook demonstra técnicas avançadas de análise dos dados indexados, incluindo clustering, análise de sentimentos e detecção de padrões.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Objetivos\\n\",\n",
    "    \"- Aplicar técnicas de machine learning nos dados indexados\\n\",\n",
    "    \"- Identificar clusters e padrões nos documentos\\n\",\n",
    "    \"- Análise temporal e de tendências\\n\",\n",
    "    \"- Geração de recomendações inteligentes\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Imports e configurações\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"sys.path.append('..')\\n\",\n",
    "    \"\\n\",\n",
    "    \"from src.search.search_engine import IntelligentSearch\\n\",\n",
    "    \"from src.search.result_processor import SearchResultProcessor\\n\",\n",
    "    \"from config.azure_config import config\\n\",\n",
    "    \"\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Machine Learning imports\\n\",\n",
    "    \"from sklearn.feature_extraction.text import TfidfVectorizer\\n\",\n",
    "    \"from sklearn.cluster import KMeans\\n\",\n",
    "    \"from sklearn.decomposition import PCA\\n\",\n",
    "    \"from sklearn.manifold import TSNE\\n\",\n",
    "    \"from sklearn.metrics.pairwise import cosine_similarity\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Text processing\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"from nltk.corpus import stopwords\\n\",\n",
    "    \"from nltk.tokenize import word_tokenize\\n\",\n",
    "    \"from nltk.stem import SnowballStemmer\\n\",\n",
    "    \"from wordcloud import WordCloud\\n\",\n",
    "    \"\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"from datetime import datetime, timedelta\\n\",\n",
    "    \"from collections import Counter, defaultdict\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Ambiente de análise avançada configurado!\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Download NLTK data if needed\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    nltk.data.find('tokenizers/punkt')\\n\",\n",
    "    \"    nltk.data.find('corpora/stopwords')\\n\",\n",
    "    \"except LookupError:\\n\",\n",
    "    \"    print(\\\"Baixando dados NLTK...\\\")\\n\",\n",
    "    \"    nltk.download('punkt')\\n\",\n",
    "    \"    nltk.download('stopwords')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Carregamento e Preparação dos Dados\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Carregar configuração e inicializar search engine\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    with open('../data/processed/index_configuration.json', 'r') as f:\\n\",\n",
    "    \"        index_config = json.load(f)\\n\",\n",
    "    \"    index_name = index_config['index_name']\\n\",\n",
    "    \"except FileNotFoundError:\\n\",\n",
    "    \"    index_name = \\\"intelligent-documents-index\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"search_engine = IntelligentSearch(\\n\",\n",
    "    \"    service_name=config.search_service_name,\\n\",\n",
    "    \"    query_key=config.search_query_key or config.search_admin_key,\\n\",\n",
    "    \"    index_name=index_name\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Buscar todos os documentos para análise\\n\",\n",
    "    \"print(\\\"Carregando documentos para análise...\\\")\\n\",\n",
    "    \"all_docs_result = search_engine.advanced_search(\\n\",\n",
    "    \"    query=\\\"*\\\",\\n\",\n",
    "    \"    top=1000,  # Ajustar conforme necessário\\n\",\n",
    "    \"    facets=[\\\"category\\\", \\\"file_type\\\", \\\"language\\\", \\\"sentiment\\\"]\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if all_docs_result['success']:\\n\",\n",
    "    \"    documents = all_docs_result['documents']\\n\",\n",
    "    \"    total_count = all_docs_result['total_count']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"✅ Carregados {len(documents)} documentos de {total_count} total\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Converter para DataFrame para análise\\n\",\n",
    "    \"    df = pd.DataFrame(documents)\\n\",\n",
    "    \"    print(f\\\"   Colunas disponíveis: {list(df.columns)}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Estatísticas básicas\\n\",\n",
    "    \"    print(f\\\"\\\\n📊 Estatísticas Básicas:\\\")\\n\",\n",
    "    \"    print(f\\\"   Documentos únicos: {len(df)}\\\")\\n\",\n",
    "    \"    print(f\\\"   Campos com dados: {df.count().sum()}\\\")\\n\",\n",
    "    \"    print(f\\\"   Campos principais:\\\")\\n\",\n",
    "    \"    for col in ['title', 'content', 'category', 'language', 'sentiment']:\\n\",\n",
    "    \"        if col in df.columns:\\n\",\n",
    "    \"            non_null = df[col].notna().sum()\\n\",\n",
    "    \"            print(f\\\"     {col}: {non_null}/{len(df)} ({non_null/len(df)*100:.1f}%)\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"❌ Erro ao carregar documentos: {all_docs_result.get('error')}\\\")\\n\",\n",
    "    \"    documents = []\\n\",\n",
    "    \"    df = pd.DataFrame()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Análise Temporal e Tendências\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if not df.empty and 'modified_date' in df.columns:\\n\",\n",
    "    \"    # Processar datas\\n\",\n",
    "    \"    df['modified_date_parsed'] = pd.to_datetime(df['modified_date'], errors='coerce')\\n\",\n",
    "    \"    df_with_dates = df.dropna(subset=['modified_date_parsed'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(df_with_dates) > 0:\\n\",\n",
    "    \"        print(f\\\"\\\\n📅 Análise Temporal de {len(df_with_dates)} documentos com datas válidas\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Adicionar colunas de tempo\\n\",\n",
    "    \"        df_with_dates['year'] = df_with_dates['modified_date_parsed'].dt.year\\n\",\n",
    "    \"        df_with_dates['month'] = df_with_dates['modified_date_parsed'].dt.month\\n\",\n",
    "    \"        df_with_dates['year_month'] = df_with_dates['modified_date_parsed'].dt.to_period('M')\\n\",\n",
    "    \"        df_with_dates['weekday'] = df_with_dates['modified_date_parsed'].dt.day_name()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Análise temporal\\n\",\n",
    "    \"        fig = make_subplots(\\n\",\n",
    "    \"            rows=2, cols=2,\\n\",\n",
    "    \"            subplot_titles=('Documentos por Mês', 'Documentos por Ano', \\n\",\n",
    "    \"                           'Documentos por Dia da Semana', 'Tendência Temporal'),\\n\",\n",
    "    \"            specs=[[{\\\"type\\\": \\\"scatter\\\"}, {\\\"type\\\": \\\"bar\\\"}],\\n\",\n",
    "    \"                   [{\\\"type\\\": \\\"bar\\\"}, {\\\"type\\\": \\\"scatter\\\"}]]\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Gráfico 1: Por mês\\n\",\n",
    "    \"        monthly_counts = df_with_dates['year_month'].value_counts().sort_index()\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Scatter(x=monthly_counts.index.astype(str), y=monthly_counts.values,\\n\",\n",
    "    \"                      mode='lines+markers', name='Por Mês'),\\n\",\n",
    "    \"            row=1, col=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Gráfico 2: Por ano\\n\",\n",
    "    \"        yearly_counts = df_with_dates['year'].value_counts().sort_index()\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Bar(x=yearly_counts.index, y=yearly_counts.values, name='Por Ano'),\\n\",\n",
    "    \"            row=1, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Gráfico 3: Por dia da semana\\n\",\n",
    "    \"        weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\\n\",\n",
    "    \"        weekday_counts = df_with_dates['weekday'].value_counts().reindex(weekday_order, fill_value=0)\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Bar(x=weekday_counts.index, y=weekday_counts.values, name='Por Dia'),\\n\",\n",
    "    \"            row=2, col=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Gráfico 4: Tendência com média móvel\\n\",\n",
    "    \"        daily_counts = df_with_dates.groupby(df_with_dates['modified_date_parsed'].dt.date).size()\\n\",\n",
    "    \"        daily_counts = daily_counts.reindex(pd.date_range(daily_counts.index.min(), \\n\",\n",
    "    \"                                                          daily_counts.index.max()), fill_value=0)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Média móvel de 7 dias\\n\",\n",
    "    \"        rolling_mean = daily_counts.rolling(window=7, center=True).mean()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Scatter(x=daily_counts.index, y=daily_counts.values,\\n\",\n",
    "    \"                      mode='markers', name='Diário', opacity=0.6),\\n\",\n",
    "    \"            row=2, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Scatter(x=rolling_mean.index, y=rolling_mean.values,\\n\",\n",
    "    \"                      mode='lines', name='Média Móvel 7d'),\\n\",\n",
    "    \"            row=2, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.update_layout(height=800, title_text=\\\"Análise Temporal dos Documentos\\\")\\n\",\n",
    "    \"        fig.show()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Insights temporais\\n\",\n",
    "    \"        print(f\\\"\\\\n🔍 Insights Temporais:\\\")\\n\",\n",
    "    \"        print(f\\\"   Período analisado: {df_with_dates['modified_date_parsed'].min().date()} a {df_with_dates['modified_date_parsed'].max().date()}\\\")\\n\",\n",
    "    \"        print(f\\\"   Mês com mais documentos: {monthly_counts.idxmax()} ({monthly_counts.max()} docs)\\\")\\n\",\n",
    "    \"        print(f\\\"   Dia da semana mais comum: {weekday_counts.idxmax()} ({weekday_counts.max()} docs)\\\")\\n\",\n",
    "    \"        print(f\\\"   Média de documentos por dia: {daily_counts.mean():.1f}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"⚠️ Nenhuma data válida encontrada para análise temporal\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"⚠️ Campo 'modified_date' não disponível para análise temporal\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Análise de Clustering de Documentos\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if not df.empty and 'content' in df.columns:\\n\",\n",
    "    \"    # Preparar textos para análise\\n\",\n",
    "    \"    texts = df['content'].fillna('').astype(str)\\n\",\n",
    "    \"    valid_texts = [text for text in texts if len(text.strip()) > 50]  # Filtrar textos muito curtos\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n🔤 Preparando {len(valid_texts)} textos para clustering...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(valid_texts) >= 10:  # Precisa de pelo menos 10 documentos\\n\",\n",
    "    \"        # Configurar TF-IDF\\n\",\n",
    "    \"        portuguese_stopwords = set(stopwords.words('portuguese'))\\n\",\n",
    "    \"        english_stopwords = set(stopwords.words('english'))\\n\",\n",
    "    \"        all_stopwords = portuguese_stopwords.union(english_stopwords)\\n# filepath: notebooks/04_advanced_analytics.ipynb\n",
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Análise Avançada e Machine Learning\\n\",\n",
    "    \"\\n\",\n",
    "    \"Este notebook demonstra técnicas avançadas de análise dos dados indexados, incluindo clustering, análise de sentimentos e detecção de padrões.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Objetivos\\n\",\n",
    "    \"- Aplicar técnicas de machine learning nos dados indexados\\n\",\n",
    "    \"- Identificar clusters e padrões nos documentos\\n\",\n",
    "    \"- Análise temporal e de tendências\\n\",\n",
    "    \"- Geração de recomendações inteligentes\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Imports e configurações\\n\",\n",
    "    \"import sys\\n\",\n",
    "    \"import os\\n\",\n",
    "    \"sys.path.append('..')\\n\",\n",
    "    \"\\n\",\n",
    "    \"from src.search.search_engine import IntelligentSearch\\n\",\n",
    "    \"from src.search.result_processor import SearchResultProcessor\\n\",\n",
    "    \"from config.azure_config import config\\n\",\n",
    "    \"\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"import plotly.express as px\\n\",\n",
    "    \"import plotly.graph_objects as go\\n\",\n",
    "    \"from plotly.subplots import make_subplots\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Machine Learning imports\\n\",\n",
    "    \"from sklearn.feature_extraction.text import TfidfVectorizer\\n\",\n",
    "    \"from sklearn.cluster import KMeans\\n\",\n",
    "    \"from sklearn.decomposition import PCA\\n\",\n",
    "    \"from sklearn.manifold import TSNE\\n\",\n",
    "    \"from sklearn.metrics.pairwise import cosine_similarity\\n\",\n",
    "    \"from sklearn.preprocessing import StandardScaler\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Text processing\\n\",\n",
    "    \"import nltk\\n\",\n",
    "    \"from nltk.corpus import stopwords\\n\",\n",
    "    \"from nltk.tokenize import word_tokenize\\n\",\n",
    "    \"from nltk.stem import SnowballStemmer\\n\",\n",
    "    \"from wordcloud import WordCloud\\n\",\n",
    "    \"\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"from datetime import datetime, timedelta\\n\",\n",
    "    \"from collections import Counter, defaultdict\\n\",\n",
    "    \"import warnings\\n\",\n",
    "    \"warnings.filterwarnings('ignore')\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Ambiente de análise avançada configurado!\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Download NLTK data if needed\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    nltk.data.find('tokenizers/punkt')\\n\",\n",
    "    \"    nltk.data.find('corpora/stopwords')\\n\",\n",
    "    \"except LookupError:\\n\",\n",
    "    \"    print(\\\"Baixando dados NLTK...\\\")\\n\",\n",
    "    \"    nltk.download('punkt')\\n\",\n",
    "    \"    nltk.download('stopwords')\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Carregamento e Preparação dos Dados\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Carregar configuração e inicializar search engine\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    with open('../data/processed/index_configuration.json', 'r') as f:\\n\",\n",
    "    \"        index_config = json.load(f)\\n\",\n",
    "    \"    index_name = index_config['index_name']\\n\",\n",
    "    \"except FileNotFoundError:\\n\",\n",
    "    \"    index_name = \\\"intelligent-documents-index\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"search_engine = IntelligentSearch(\\n\",\n",
    "    \"    service_name=config.search_service_name,\\n\",\n",
    "    \"    query_key=config.search_query_key or config.search_admin_key,\\n\",\n",
    "    \"    index_name=index_name\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Buscar todos os documentos para análise\\n\",\n",
    "    \"print(\\\"Carregando documentos para análise...\\\")\\n\",\n",
    "    \"all_docs_result = search_engine.advanced_search(\\n\",\n",
    "    \"    query=\\\"*\\\",\\n\",\n",
    "    \"    top=1000,  # Ajustar conforme necessário\\n\",\n",
    "    \"    facets=[\\\"category\\\", \\\"file_type\\\", \\\"language\\\", \\\"sentiment\\\"]\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"if all_docs_result['success']:\\n\",\n",
    "    \"    documents = all_docs_result['documents']\\n\",\n",
    "    \"    total_count = all_docs_result['total_count']\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"✅ Carregados {len(documents)} documentos de {total_count} total\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Converter para DataFrame para análise\\n\",\n",
    "    \"    df = pd.DataFrame(documents)\\n\",\n",
    "    \"    print(f\\\"   Colunas disponíveis: {list(df.columns)}\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Estatísticas básicas\\n\",\n",
    "    \"    print(f\\\"\\\\n📊 Estatísticas Básicas:\\\")\\n\",\n",
    "    \"    print(f\\\"   Documentos únicos: {len(df)}\\\")\\n\",\n",
    "    \"    print(f\\\"   Campos com dados: {df.count().sum()}\\\")\\n\",\n",
    "    \"    print(f\\\"   Campos principais:\\\")\\n\",\n",
    "    \"    for col in ['title', 'content', 'category', 'language', 'sentiment']:\\n\",\n",
    "    \"        if col in df.columns:\\n\",\n",
    "    \"            non_null = df[col].notna().sum()\\n\",\n",
    "    \"            print(f\\\"     {col}: {non_null}/{len(df)} ({non_null/len(df)*100:.1f}%)\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"❌ Erro ao carregar documentos: {all_docs_result.get('error')}\\\")\\n\",\n",
    "    \"    documents = []\\n\",\n",
    "    \"    df = pd.DataFrame()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Análise Temporal e Tendências\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if not df.empty and 'modified_date' in df.columns:\\n\",\n",
    "    \"    # Processar datas\\n\",\n",
    "    \"    df['modified_date_parsed'] = pd.to_datetime(df['modified_date'], errors='coerce')\\n\",\n",
    "    \"    df_with_dates = df.dropna(subset=['modified_date_parsed'])\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(df_with_dates) > 0:\\n\",\n",
    "    \"        print(f\\\"\\\\n📅 Análise Temporal de {len(df_with_dates)} documentos com datas válidas\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Adicionar colunas de tempo\\n\",\n",
    "    \"        df_with_dates['year'] = df_with_dates['modified_date_parsed'].dt.year\\n\",\n",
    "    \"        df_with_dates['month'] = df_with_dates['modified_date_parsed'].dt.month\\n\",\n",
    "    \"        df_with_dates['year_month'] = df_with_dates['modified_date_parsed'].dt.to_period('M')\\n\",\n",
    "    \"        df_with_dates['weekday'] = df_with_dates['modified_date_parsed'].dt.day_name()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Análise temporal\\n\",\n",
    "    \"        fig = make_subplots(\\n\",\n",
    "    \"            rows=2, cols=2,\\n\",\n",
    "    \"            subplot_titles=('Documentos por Mês', 'Documentos por Ano', \\n\",\n",
    "    \"                           'Documentos por Dia da Semana', 'Tendência Temporal'),\\n\",\n",
    "    \"            specs=[[{\\\"type\\\": \\\"scatter\\\"}, {\\\"type\\\": \\\"bar\\\"}],\\n\",\n",
    "    \"                   [{\\\"type\\\": \\\"bar\\\"}, {\\\"type\\\": \\\"scatter\\\"}]]\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Gráfico 1: Por mês\\n\",\n",
    "    \"        monthly_counts = df_with_dates['year_month'].value_counts().sort_index()\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Scatter(x=monthly_counts.index.astype(str), y=monthly_counts.values,\\n\",\n",
    "    \"                      mode='lines+markers', name='Por Mês'),\\n\",\n",
    "    \"            row=1, col=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Gráfico 2: Por ano\\n\",\n",
    "    \"        yearly_counts = df_with_dates['year'].value_counts().sort_index()\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Bar(x=yearly_counts.index, y=yearly_counts.values, name='Por Ano'),\\n\",\n",
    "    \"            row=1, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Gráfico 3: Por dia da semana\\n\",\n",
    "    \"        weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\\n\",\n",
    "    \"        weekday_counts = df_with_dates['weekday'].value_counts().reindex(weekday_order, fill_value=0)\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Bar(x=weekday_counts.index, y=weekday_counts.values, name='Por Dia'),\\n\",\n",
    "    \"            row=2, col=1\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Gráfico 4: Tendência com média móvel\\n\",\n",
    "    \"        daily_counts = df_with_dates.groupby(df_with_dates['modified_date_parsed'].dt.date).size()\\n\",\n",
    "    \"        daily_counts = daily_counts.reindex(pd.date_range(daily_counts.index.min(), \\n\",\n",
    "    \"                                                          daily_counts.index.max()), fill_value=0)\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Média móvel de 7 dias\\n\",\n",
    "    \"        rolling_mean = daily_counts.rolling(window=7, center=True).mean()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Scatter(x=daily_counts.index, y=daily_counts.values,\\n\",\n",
    "    \"                      mode='markers', name='Diário', opacity=0.6),\\n\",\n",
    "    \"            row=2, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        fig.add_trace(\\n\",\n",
    "    \"            go.Scatter(x=rolling_mean.index, y=rolling_mean.values,\\n\",\n",
    "    \"                      mode='lines', name='Média Móvel 7d'),\\n\",\n",
    "    \"            row=2, col=2\\n\",\n",
    "    \"        )\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        fig.update_layout(height=800, title_text=\\\"Análise Temporal dos Documentos\\\")\\n\",\n",
    "    \"        fig.show()\\n\",\n",
    "    \"        \\n\",\n",
    "    \"        # Insights temporais\\n\",\n",
    "    \"        print(f\\\"\\\\n🔍 Insights Temporais:\\\")\\n\",\n",
    "    \"        print(f\\\"   Período analisado: {df_with_dates['modified_date_parsed'].min().date()} a {df_with_dates['modified_date_parsed'].max().date()}\\\")\\n\",\n",
    "    \"        print(f\\\"   Mês com mais documentos: {monthly_counts.idxmax()} ({monthly_counts.max()} docs)\\\")\\n\",\n",
    "    \"        print(f\\\"   Dia da semana mais comum: {weekday_counts.idxmax()} ({weekday_counts.max()} docs)\\\")\\n\",\n",
    "    \"        print(f\\\"   Média de documentos por dia: {daily_counts.mean():.1f}\\\")\\n\",\n",
    "    \"        \\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        print(\\\"⚠️ Nenhuma data válida encontrada para análise temporal\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"⚠️ Campo 'modified_date' não disponível para análise temporal\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Análise de Clustering de Documentos\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"if not df.empty and 'content' in df.columns:\\n\",\n",
    "    \"    # Preparar textos para análise\\n\",\n",
    "    \"    texts = df['content'].fillna('').astype(str)\\n\",\n",
    "    \"    valid_texts = [text for text in texts if len(text.strip()) > 50]  # Filtrar textos muito curtos\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    print(f\\\"\\\\n🔤 Preparando {len(valid_texts)} textos para clustering...\\\")\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    if len(valid_texts) >= 10:  # Precisa de pelo menos 10 documentos\\n\",\n",
    "    \"        # Configurar TF-IDF\\n\",\n",
    "    \"        portuguese_stopwords = set(stopwords.words('portuguese'))\\n\",\n",
    "    \"        english_stopwords = set(stopwords.words('english'))\\n\",\n",
    "    \"        all_stopwords = portuguese_stopwords.union(english_stopwords)\\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
